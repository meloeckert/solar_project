{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831245bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load images\n",
    "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers, optimizers, callbacks\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fbb8a2",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd969b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_data_dir = \"raw_data/training-data-exp/\"\n",
    "#test_data_dir = \"raw_data/test-data-binary/\"\n",
    "\n",
    "train_ds = image_dataset_from_directory(\n",
    "  train_data_dir,\n",
    "  labels = \"inferred\",\n",
    "  label_mode = \"binary\",\n",
    "  #class_names = ['clean','dust'],  \n",
    "  seed=123,\n",
    "  image_size=(225, 225),\n",
    "  batch_size=batch_size,\n",
    "  validation_split=0.2,\n",
    "  subset='both'\n",
    ") \n",
    "\n",
    "train_data = train_ds[0]\n",
    "val_data = train_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7307dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = val_data.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03df9dc",
   "metadata": {},
   "source": [
    "### Binary model: Clean vs Snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72a1529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a model with the same convolutional layers, but we'll add Augmentation layers before that\n",
    "\n",
    "model_clean_snow = Sequential()\n",
    "\n",
    "model_clean_snow.add(layers.Rescaling(1./255, input_shape = (225, 225, 3)))\n",
    "\n",
    "# Data Augmentation Layers\n",
    "\n",
    "model_clean_snow.add(layers.RandomFlip(\"horizontal\"))\n",
    "model_clean_snow.add(layers.RandomZoom(0.1))\n",
    "model_clean_snow.add(layers.RandomTranslation(0.2, 0.2))\n",
    "model_clean_snow.add(layers.RandomRotation(0.1))\n",
    "\n",
    "\n",
    "# Convolutional Layers\n",
    "\n",
    "model_clean_snow.add(layers.Conv2D(filters = 32, kernel_size = (3,3), activation=\"relu\", padding = \"same\"))\n",
    "model_clean_snow.add(layers.MaxPooling2D(pool_size=(2, 2), padding = \"same\") )\n",
    "\n",
    "\n",
    "model_clean_snow.add(layers.Conv2D(filters = 32, kernel_size = (3,3), input_shape = (225, 225, 3), activation=\"relu\", padding = \"same\"))\n",
    "model_clean_snow.add(layers.MaxPooling2D(pool_size=(2, 2), padding = \"same\") )\n",
    "\n",
    "\n",
    "model_clean_snow.add(layers.Conv2D(filters = 64, kernel_size = (3,3), input_shape = (225, 225, 3), activation=\"relu\", padding = \"same\"))\n",
    "model_clean_snow.add(layers.MaxPooling2D(pool_size=(2, 2), padding = \"same\") )\n",
    "\n",
    "model_clean_snow.add(layers.Conv2D(filters = 128, kernel_size = (3,3), input_shape = (225, 225, 3), activation=\"relu\", padding = \"same\"))\n",
    "model_clean_snow.add(layers.MaxPooling2D(pool_size=(2, 2), padding = \"same\") )\n",
    "\n",
    "model_clean_snow.add(layers.Flatten())\n",
    "\n",
    "model_clean_snow.add(layers.Dense(64, activation=\"relu\"))\n",
    "\n",
    "model_clean_snow.add(layers.Dropout(0.5))\n",
    "\n",
    "model_clean_snow.add(layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81f17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "model_clean_snow.compile(loss= 'binary_crossentropy', #'categorical_crossentropy',\n",
    "              optimizer= adam,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3482aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"model_clean_damage\"\n",
    "\n",
    "modelCheckpoint = callbacks.ModelCheckpoint(\"{}.h5\".format(MODEL), monitor=\"val_loss\", verbose=0, save_best_only=True)\n",
    "\n",
    "LRreducer = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor = 0.1, patience=3, verbose=1, min_lr=0)\n",
    "\n",
    "EarlyStopper = callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978a24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history_clean_snow = model_clean_snow.fit(\n",
    "        train_data,\n",
    "        epochs=30,\n",
    "        validation_data=val_data,\n",
    "        callbacks = [modelCheckpoint, LRreducer, EarlyStopper]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11593472",
   "metadata": {},
   "source": [
    "### Pred tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf14b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictImage_binary(url, model):\n",
    "\n",
    "  # Takes an imafe and a model\n",
    "\n",
    "  img = url\n",
    "  img = img_to_array(img)\n",
    "  img = img.reshape((-1, 225, 225, 3))\n",
    "  res = model.predict(img)\n",
    "  print(f\"Probabilities: \")\n",
    "  print(f\"{res[0]}\")  \n",
    "  res = model.predict(img)[0][0]\n",
    "  if(res < 0.5):\n",
    "    pred_class = \"clean\"\n",
    "    prob = 1-res\n",
    "  if(res >= 0.5):\n",
    "    pred_class = \"damage\"\n",
    "    prob = res\n",
    "\n",
    "  print(\"Class : \", pred_class)\n",
    "  print(\"probability = \",prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictImage_multiclass(url, model):\n",
    "\n",
    "  # Takes an image and a model\n",
    "\n",
    "  img = url\n",
    "  img = img_to_array(img)\n",
    "  img = img.reshape((-1, 225, 225, 3))\n",
    "  res = model.predict(img)\n",
    "  print(f\"Probabilities: \")\n",
    "  names_of_classes = class_names\n",
    "  print(f\"{names_of_classes}\")\n",
    "  print(f\"{res[0]}\")\n",
    "  print(f\"Result: {names_of_classes[find_index_of_max_element(res[0].tolist())]}\")  \n",
    "  return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d7c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_clean_1 = load_img(f\"raw_data/training-data/clean/Cleaan (4).jpeg\", target_size=(225, 225))\n",
    "predictImage_binary(img_clean_1,model_clean_snow)\n",
    "plt.imshow(img_clean_1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb69115",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_clean_2 = load_img(f\"raw_data/training-data/clean/Cleaan (15).jpeg\", target_size=(225, 225))\n",
    "predictImage_binary(img_clean_2,model_clean_snow)\n",
    "plt.imshow(img_clean_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be72b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_clean_2 = load_img(f\"raw_data/training-data/solar/Solar (15).jpeg\", target_size=(225, 225))\n",
    "predictImage_binary(img_clean_2,model_clean_snow)\n",
    "plt.imshow(img_clean_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ccd339",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_clean_2 = load_img(f\"raw_data/training-data/solar/Solar (21).jpeg\", target_size=(225, 225))\n",
    "predictImage_binary(img_clean_2,model_clean_snow)\n",
    "plt.imshow(img_clean_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652c5001",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_clean_2 = load_img(f\"raw_data/training-data/solar/Solar (28).jpeg\", target_size=(225, 225))\n",
    "predictImage_binary(img_clean_2,model_clean_snow)\n",
    "plt.imshow(img_clean_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e85c8",
   "metadata": {},
   "source": [
    "## Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca3c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.math import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "predictions = np.array([])\n",
    "probabilities = np.array([])\n",
    "labels =  np.array([])\n",
    "\n",
    "for x, y in val_data:\n",
    "    \n",
    "  proba = model_clean_snow.predict(x)\n",
    "  pred = np.where(proba < 0.5, 0, 1).reshape(len(y))\n",
    "    \n",
    "  predictions = np.concatenate([predictions, pred])\n",
    "\n",
    "  label =  np.squeeze(y) #y.reshape(len(pred))\n",
    "  proba_damage = np.squeeze(1- proba)  \n",
    "  prob = proba.reshape(len(y))\n",
    "\n",
    "  probabilities = np.concatenate([probabilities, proba_damage])  \n",
    "\n",
    "  labels = np.concatenate([labels, label])  #np.argmax(y.numpy()\n",
    "\n",
    "report = classification_report(labels, predictions, target_names=class_names)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da3edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(labels=labels, predictions=predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48141e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065c7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "precision, recall, threshold = precision_recall_curve(labels, probabilities)\n",
    "scores = pd.DataFrame({'threshold':threshold,\n",
    "                       'precision': precision[:-1],\n",
    "                       'recall':recall[:-1]}) # Store in a dataframe\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719fbf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores['recall'],scores['precision'])\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c385fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_threshold = scores[scores['precision'] >= 0.4].threshold.min()\n",
    "new_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c935d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_predict(X, custom_threshold, model):\n",
    "    probability = model.predict(X) # Get likelihood of each sample being classified as 0 or 1\n",
    "    proba_damage = probability\n",
    "    #print(modified_array)\n",
    "    #more_5y_probs = probability[:, 1] # Only keep expensive likelihoods (1) \n",
    "    return (proba_damage > custom_threshold).astype(int) # Boolean outcome converted to 0 or 1\n",
    "\n",
    "pred_thres = np.array([])\n",
    "for x, y in val_data:    \n",
    "    updated_preds = custom_predict(X=x, custom_threshold=new_threshold, model=model_clean_snow) # Update prediction\n",
    "    updated_preds = np.squeeze(updated_preds)\n",
    "    pred_thres = np.concatenate([pred_thres, updated_preds]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d10d0",
   "metadata": {},
   "source": [
    "## Threshold adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dde820",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(labels=labels, predictions=pred_thres).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe456fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "report = classification_report(labels, pred_thres, target_names=class_names)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2396aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for label, pred in zip(labels, pred_thres):\n",
    "#    print(f'label: {label}, pred: {pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a63f7f",
   "metadata": {},
   "source": [
    "### Combining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d568ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for combining models\n",
    "\n",
    "def combine_models(model1,model2,image)\n",
    "    prediction = model1.predict(image)\n",
    "\n",
    "    if prediction!='dirt':\n",
    "        prediction = model2(image)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c180e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the .h5 file\n",
    "model1 = load_model('model_filename.h5')\n",
    "\n",
    "# Now you can use the loaded model for predictions or further training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8906e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2b8b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
