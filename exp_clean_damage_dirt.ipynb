{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c0a9a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load images\n",
    "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers, optimizers, callbacks\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from PIL import Image\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55f65fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 636 files belonging to 3 classes.\n",
      "Using 509 files for training.\n",
      "Using 127 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "train_data_dir = \"raw_data/training-data-exp/\"\n",
    "#test_data_dir = \"raw_data/test-data-binary/\"\n",
    "\n",
    "train_ds = image_dataset_from_directory(\n",
    "  train_data_dir,\n",
    "  labels = \"inferred\",\n",
    "  label_mode = \"int\",  \n",
    "  seed=123,\n",
    "  image_size=(225, 225),\n",
    "  batch_size=batch_size,\n",
    "  validation_split=0.2,\n",
    "  subset='both'\n",
    ") \n",
    "\n",
    "train_data = train_ds[0]\n",
    "val_data = train_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf02282f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clean', 'damage', 'dirt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_data.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca9d62",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45901ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "# We'll use a model with the same convolutional layers, but we'll add Augmentation layers before that\n",
    "\n",
    "model_multi = Sequential()\n",
    "\n",
    "model_multi.add(layers.Rescaling(1./255, input_shape = (225, 225, 3)))\n",
    "\n",
    "# Data Augmentation Layers\n",
    "\n",
    "model_multi.add(layers.RandomFlip(\"horizontal\"))\n",
    "model_multi.add(layers.RandomZoom(0.1))\n",
    "model_multi.add(layers.RandomTranslation(0.2, 0.2))\n",
    "model_multi.add(layers.RandomRotation(0.1))\n",
    "\n",
    "\n",
    "# Convolutional Layers\n",
    "\n",
    "model_multi.add(layers.Conv2D(filters = 32, kernel_size = (3,3), activation=\"relu\", padding = \"same\"))\n",
    "model_multi.add(layers.MaxPooling2D(pool_size=(2, 2), padding = \"same\") )\n",
    "\n",
    "\n",
    "model_multi.add(layers.Conv2D(filters = 32, kernel_size = (3,3), input_shape = (225, 225, 3), activation=\"relu\", padding = \"same\"))\n",
    "model_multi.add(layers.MaxPooling2D(pool_size=(2, 2), padding = \"same\") )\n",
    "\n",
    "\n",
    "model_multi.add(layers.Conv2D(filters = 64, kernel_size = (3,3), input_shape = (225, 225, 3), activation=\"relu\", padding = \"same\"))\n",
    "model_multi.add(layers.MaxPooling2D(pool_size=(2, 2), padding = \"same\") )\n",
    "\n",
    "model_multi.add(layers.Conv2D(filters = 128, kernel_size = (3,3), input_shape = (225, 225, 3), activation=\"relu\", padding = \"same\"))\n",
    "model_multi.add(layers.MaxPooling2D(pool_size=(2, 2), padding = \"same\") )\n",
    "\n",
    "model_multi.add(layers.Flatten())\n",
    "\n",
    "model_multi.add(layers.Dense(64, activation=\"relu\"))\n",
    "\n",
    "model_multi.add(layers.Dropout(0.5))\n",
    "\n",
    "model_multi.add(layers.Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0625e963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 225, 225, 3)       0         \n",
      "                                                                 \n",
      " random_flip (RandomFlip)    (None, 225, 225, 3)       0         \n",
      "                                                                 \n",
      " random_zoom (RandomZoom)    (None, 225, 225, 3)       0         \n",
      "                                                                 \n",
      " random_translation (RandomT  (None, 225, 225, 3)      0         \n",
      " ranslation)                                                     \n",
      "                                                                 \n",
      " random_rotation (RandomRota  (None, 225, 225, 3)      0         \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 225, 225, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 113, 113, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 113, 113, 32)      9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 57, 57, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 57, 57, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 29, 29, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 29, 29, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 15, 15, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 28800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1843264   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,945,955\n",
      "Trainable params: 1,945,955\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_multi.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0afd97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "model_multi.compile(loss= SparseCategoricalCrossentropy(),\n",
    "              optimizer= adam,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a98d5103",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"model_multi\"\n",
    "\n",
    "modelCheckpoint = callbacks.ModelCheckpoint(\"{}.h5\".format(MODEL), monitor=\"val_loss\", verbose=0, save_best_only=True)\n",
    "\n",
    "LRreducer = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor = 0.1, patience=3, verbose=1, min_lr=0)\n",
    "\n",
    "EarlyStopper = callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21e6bddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "32/32 [==============================] - 26s 649ms/step - loss: 3.4911 - accuracy: 0.3084 - val_loss: 1.0894 - val_accuracy: 0.5591 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 29s 852ms/step - loss: 3.4009 - accuracy: 0.4126 - val_loss: 1.1470 - val_accuracy: 0.3386 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 25s 756ms/step - loss: 3.3451 - accuracy: 0.2809 - val_loss: 1.0041 - val_accuracy: 0.3386 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 24s 718ms/step - loss: 3.3251 - accuracy: 0.4401 - val_loss: 0.9923 - val_accuracy: 0.5827 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 40s 1s/step - loss: 3.2230 - accuracy: 0.4361 - val_loss: 1.0187 - val_accuracy: 0.5512 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 25s 736ms/step - loss: 3.2207 - accuracy: 0.4872 - val_loss: 0.9753 - val_accuracy: 0.5433 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 25s 739ms/step - loss: 3.2674 - accuracy: 0.3929 - val_loss: 1.0633 - val_accuracy: 0.3543 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 25s 745ms/step - loss: 3.2686 - accuracy: 0.4067 - val_loss: 0.9186 - val_accuracy: 0.6220 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 26s 797ms/step - loss: 3.2028 - accuracy: 0.4558 - val_loss: 0.9488 - val_accuracy: 0.5591 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 30s 928ms/step - loss: 3.1002 - accuracy: 0.4794 - val_loss: 1.1218 - val_accuracy: 0.4252 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.2986 - accuracy: 0.3556\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "32/32 [==============================] - 26s 792ms/step - loss: 3.2986 - accuracy: 0.3556 - val_loss: 1.0424 - val_accuracy: 0.3071 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 24s 726ms/step - loss: 3.3033 - accuracy: 0.3104 - val_loss: 1.0379 - val_accuracy: 0.3150 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 26s 799ms/step - loss: 3.2533 - accuracy: 0.3281 - val_loss: 1.0377 - val_accuracy: 0.3307 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.1696 - accuracy: 0.3281\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "32/32 [==============================] - 44s 1s/step - loss: 3.1696 - accuracy: 0.3281 - val_loss: 1.0752 - val_accuracy: 0.3386 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 24s 743ms/step - loss: 3.1908 - accuracy: 0.3143 - val_loss: 1.0672 - val_accuracy: 0.3386 - lr: 1.0000e-05\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 25s 774ms/step - loss: 3.1723 - accuracy: 0.3222 - val_loss: 1.0658 - val_accuracy: 0.3307 - lr: 1.0000e-05\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.1713 - accuracy: 0.3261\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "32/32 [==============================] - 25s 750ms/step - loss: 3.1713 - accuracy: 0.3261 - val_loss: 1.0678 - val_accuracy: 0.3386 - lr: 1.0000e-05\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 25s 738ms/step - loss: 3.1889 - accuracy: 0.3183 - val_loss: 1.0678 - val_accuracy: 0.3307 - lr: 1.0000e-06\n",
      "CPU times: user 47min 29s, sys: 2min 50s, total: 50min 19s\n",
      "Wall time: 8min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#'clean', 'damage', 'dirt'\n",
    "history_multi = model_multi.fit(\n",
    "        train_data,\n",
    "        epochs=30,\n",
    "        validation_data=val_data,\n",
    "        callbacks = [modelCheckpoint, LRreducer, EarlyStopper],\n",
    "        class_weight = {\n",
    "            0: 3.0,\n",
    "            1: 6.0,\n",
    "            2: 2.0\n",
    "        }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba0207eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06999999999999995"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "improvement = (0.31 - (1/3))/(1/3)\n",
    "improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9578c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax[0].set_title('loss')\n",
    "    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n",
    "    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "    ax[1].set_title('accuracy')\n",
    "    ax[1].plot(history.epoch, history.history[\"accuracy\"], label=\"Train acc\")\n",
    "    ax[1].plot(history.epoch, history.history[\"val_accuracy\"], label=\"Validation acc\")\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fcc943",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834a0978",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0776e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_of_max_element(input_list):\n",
    "    max_value = max(input_list)\n",
    "    max_index = input_list.index(max_value)\n",
    "    return max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b900fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictImage_multiclass(url, model):\n",
    "\n",
    "  # Takes an image and a model\n",
    "\n",
    "  img = url\n",
    "  img = img_to_array(img)\n",
    "  img = img.reshape((-1, 225, 225, 3))\n",
    "  res = model.predict(img)\n",
    "  print(f\"Probabilities: \")\n",
    "  names_of_classes = class_names\n",
    "  print(f\"{names_of_classes}\")\n",
    "  print(f\"{res[0]}\")\n",
    "  print(f\"Result: {names_of_classes[find_index_of_max_element(res[0].tolist())]}\")  \n",
    "  return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d612f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_clean_1 = load_img(f\"raw_data/training-data/clean/Cleaan (4).jpeg\", target_size=(225, 225))\n",
    "predictImage_multiclass(img_clean_1,model_multi)\n",
    "plt.imshow(img_clean_1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8354bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_clean_1 = load_img(f\"raw_data/training-data/clean/Cleaan (12).jpeg\", target_size=(225, 225))\n",
    "predictImage_multiclass(img_clean_1,model_multi)\n",
    "plt.imshow(img_clean_1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551e2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_clean_1 = load_img(f\"raw_data/training-data/clean/Cleaan (21).jpeg\", target_size=(225, 225))\n",
    "predictImage_multiclass(img_clean_1,model_multi)\n",
    "plt.imshow(img_clean_1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99213d",
   "metadata": {},
   "source": [
    "### Snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_snow = load_img(f\"raw_data/training-data/solar/Solar (3).jpeg\", target_size=(225, 225))\n",
    "predictImage_multiclass(img_snow,model_multi)\n",
    "plt.imshow(img_snow);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f381bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_snow = load_img(f\"raw_data/training-data/solar/Solar (12).jpeg\", target_size=(225, 225))\n",
    "predictImage_multiclass(img_snow,model_multi)\n",
    "plt.imshow(img_snow);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_snow = load_img(f\"raw_data/training-data/solar/Solar (33).jpeg\", target_size=(225, 225))\n",
    "predictImage_multiclass(img_snow,model_multi)\n",
    "plt.imshow(img_snow);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e4d262",
   "metadata": {},
   "source": [
    "### Damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655d8441",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_electrical = load_img(f\"raw_data/training-data/electrical/Electrical (29).jpeg\", target_size=(225, 225))\n",
    "predictImage_multiclass(img_electrical,model_multi)\n",
    "plt.imshow(img_electrical);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222929a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_physical = load_img(f\"raw_data/training-data/physical_damaged/Physical-damaged (35).jpeg\", target_size=(225, 225))\n",
    "predictImage_multiclass(img_physical,model_multi)\n",
    "plt.imshow(img_physical);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_physical = load_img(f\"raw_data/training-data/physical_damaged/Physical-damaged (37).jpeg\", target_size=(225, 225))\n",
    "predictImage_multiclass(img_physical,model_multi)\n",
    "plt.imshow(img_physical);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d774fa3",
   "metadata": {},
   "source": [
    "### Bird or dust\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d64f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##pics bird 37,55,59,  --> macro pic on brid drop --> predicting wrong class\n",
    "# pic 65\n",
    "img_physical = load_img(f\"raw_data/training-data/bird/Bird (37).jpeg\", target_size=(225, 225))\n",
    "predictImage_multiclass(img_physical,model_multi)\n",
    "plt.imshow(img_physical);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca7a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_physical = load_img(f\"raw_data/training-data/bird/Bird (5).jpeg\", target_size=(225, 225))\n",
    "predictImage_multiclass(img_physical,model_multi)\n",
    "plt.imshow(img_physical);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f9cb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_physical = load_img(f\"raw_data/training-data/dust/Dust (37).jpeg\", target_size=(225, 225))\n",
    "predictImage_multiclass(img_physical,model_multi)\n",
    "plt.imshow(img_physical);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
